{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAIC-Cambridge: Distributed AI Group\n",
    "## ML Engineer - Coding Challenge\n",
    "Thank you for taking the time to complete this programming assignment and congratulations for reaching this stage of the interview process! \n",
    "\n",
    "\n",
    "This assignment is based on `PyTorch` and consists of 5 tasks, all based on the same *object detection* model, provided below. \n",
    "The model is obtained from `torchvision` and all code provided is just for reference purposes. You are free to change any parts of the given code, while implementing the requested features.\n",
    "\n",
    "Keep in mind that:\n",
    "- High-quality software architecture and coding practices are of high priority. Consider that the code would be used across projects in SAIC-C.\n",
    "- Detection performance, end-to-end inference latency and efficiency are all of utmost importance across tasks.\n",
    "- Don't forget to document any assumptions and design choices that you made and for each solution that you provide, as well as comment on its limitations.\n",
    "- In case you experiment with multiple solutions for a given task, feel free to discuss all the attempts and the underlying trade-offs on your report. \n",
    "\n",
    ">The deliverable of this assignment is a .zip file that should include:\n",
    ">- an interactive report based on a single Jupyter Notebook where all solutions/results will be clearly documented and can be reproduced; and\n",
    ">- a directory with the source code that implements the assignment, following a structure of your choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referene Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import detection\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.io.image import read_image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(dev)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "transforms = []\n",
    "transforms.append(T.ToTensor())\n",
    "#transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "transforms = T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.download(\"https://alk15.github.io/home/files/img1.jpg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = Image.open('img1.jpg').convert(\"RGB\")\n",
    "x = transforms(x)\n",
    "x = x.unsqueeze(0) \n",
    "x = x.to(dev)\n",
    "\n",
    "#Run Inference\n",
    "with torch.no_grad():\n",
    "    prediction = model(x)[0]\n",
    "\n",
    "scores = prediction[\"scores\"].cpu().numpy()\n",
    "print('Scores:', scores)\n",
    "\n",
    "img = read_image(\"img1.jpg\")\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          colors=\"red\",\n",
    "                          width=1)\n",
    "im = to_pil_image(box.detach())\n",
    "plt.imshow(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1:\n",
    "For the first task, the provided object detection model should be applied *on all frames* of a video. The model’s predictions need to be *post-processed* to return the bounding box of the **“main” person** in each frame. \n",
    "\n",
    "The video file for this task can be downloaded from: \n",
    ">https://alk15.github.io/home/files/london-walk.mp4\n",
    "\n",
    "\n",
    "For the target application of the whole assignment, the output resolution should be `(WxH)=427x240`.\n",
    "\n",
    "Don’t forget that end-to-end latency plays an important role in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2:\n",
    "For this task, you will need to quantise the above model to a precision that would maximise efficiency without significant accuracy degradation.\n",
    "\n",
    "There is no expectation to perform any fine-tuning on the model. \n",
    "If you experiment with different quantisation schemes, feel free to report all of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task3:\n",
    "Here you should evaluate the impact of the quantisation scheme you applied in the previous task, in terms of all the aspects of model deployment that have been affected. \n",
    "Your analysis can be reported at the presentation format of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Task4:\n",
    "For this task, wrap the above model so it can read incoming frames from a *webcam stream* and broadcast the results using the *MQTT protocol*. The provided solution should be able to work out-of-the-box across different systems. Inference latency remains important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task5:\n",
    "For the final task, develop a client-server distributed system, where the results of the detection model are transmitted to the server (residing on the same machine). On the server side, the object detection results should be visualised through a web interface of your choice. Fancy graphic design is not required. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "interview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343332fb22214708c16cf8bf8fa77fbb975f431a08dc7d5b2dfbb4ce193e0ea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
